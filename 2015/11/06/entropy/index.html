<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Feature Selection using Mutual Information | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. H">
<meta name="keywords" content="entropy">
<meta property="og:type" content="article">
<meta property="og:title" content="Feature Selection using Mutual Information">
<meta property="og:url" content="http://luofanghao.github.io/blog/2015/11/06/entropy/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. H">
<meta property="og:updated_time" content="2016-07-01T08:44:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Feature Selection using Mutual Information">
<meta name="twitter:description" content="Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. H">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/blog/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">Home</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://luofanghao.github.io/blog"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-entropy" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2015/11/06/entropy/" class="article-date">
  <time datetime="2015-11-06T19:57:04.000Z" itemprop="datePublished">2015-11-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/éšç¬”/">éšç¬”</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Feature Selection using Mutual Information
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Feature selection is important in pratical machine learning cases, aiming to obtain the features that are more capable to classify. One principle of feature selection is based on mutual information. Here we will introduce the basic concept of this.</p>
<a id="more"></a>
<h1 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h1><h2 id="1-Shannon-Measure-of-Information-SIC"><a href="#1-Shannon-Measure-of-Information-SIC" class="headerlink" title="1. Shannon Measure of Information (SIC)"></a>1. Shannon Measure of Information (SIC)</h2><p>The measure of information should contain the following intuitive properties:</p>
<ol>
<li>Information contained in the events should be defined according to some measure of uncertainty of the events.</li>
<li>Less certain events should contain more information than more certain events.</li>
<li>The information of unrelated events taken as a single event should equal the sum of the information of the unrelated events.</li>
<li>The probabilities of various possible events should be taken into account.</li>
</ol>
<p>A natural way to measure the uncertainty of an event X is the probability of X. Based on this, according to the properties 2 â€“ 4 above, the information content in an event can be expressed as:<br>$$INFO {X} = -\log p_x = \log\frac{1}{p_x}$$<br>, which can also be extended to multiple independent events as:<br>$$SIC = \sum_xp_x\log_2\frac{1}{p_x} = -\sum_xp_x\log_2{p_x}$$</p>
<h2 id="2-Entropy"><a href="#2-Entropy" class="headerlink" title="2. Entropy"></a>2. Entropy</h2><p>Based on the derivation above, we can define entropy as:<br>$$H(X) = \sum_x{p(x)logp(x)}$$<br>According to its definition, entropy is a measure of the uncertainty of a random variable. Concretely, for a random variable, more uncertain (probability distribution is more flat) it is, larger the entropy value.</p>
<p>Similar with conditional probability, here we can also define the conditional entropy $H(Y|X)$ as the uncertainty of random variable Y when the random variable X is known.<br>$$H(Y|X) = \sum_x p(x)H(Y|X=x)$$</p>
<h2 id="3-Information-Gain"><a href="#3-Information-Gain" class="headerlink" title="3. Information Gain"></a>3. Information Gain</h2><p>In information theory, the information gain is actually called Mutual Information:</p>
<p>$$I(X, Y) = H(X) - H(X|Y)$$</p>
<p>Mutual information measures the information that X and Y share. In another words, it measures how much knowing one of these variables reduces uncertainty about the other. Concretely, if X and Y are independent, then the mutual information is zero; if X and Y are identical, then the $ğ¼(ğ‘‹,ğ‘Œ) =ğ»(ğ‘‹) =ğ»(ğ‘Œ)$</p>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><p>Letâ€™s take a machine learning case for example: X is the dataset with labels and several features and we want to find out which feature contains the most capability to classify this dataset. So what should we do to get it? By using the information gain criteria, we can just simply select the one (feature Q) with the highest mutual information $I(X,Q)$</p>
<p>A brief explanation: $H(X)$ is the uncertainty of the dataset (and its label); $H(X|Q)$ is the uncertainty of the dataset when the information Q is known; and the mutual information is the deference between them, which also can be considered as the decrease of the uncertainty of classifiation with the knowledge of Q. This is also why it is called â€œinformation gainâ€. Therefore, it is obvious that the feature with the highest information gain contains the most capability to classify the dataset.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luofanghao.github.io/blog/2015/11/06/entropy/" data-id="cj94ikx7t0007z48cki36aygx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/entropy/">entropy</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2016/06/27/caffeä½¿ç”¨æ€»ç»“/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Caffe ä½¿ç”¨æ€»ç»“1ï¼šæ•°æ®è®¿é—®
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Caffe/">Caffe</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/è®ºæ–‡ç¬”è®°/">è®ºæ–‡ç¬”è®°</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/éšç¬”/">éšç¬”</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Caffe/">Caffe</a></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Model-Compression/">Model Compression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/Semantic-Segmentation/">Semantic Segmentation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/entropy/">entropy</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/blog/tags/Caffe/" style="font-size: 20px;">Caffe</a> <a href="/blog/tags/Model-Compression/" style="font-size: 15px;">Model Compression</a> <a href="/blog/tags/Semantic-Segmentation/" style="font-size: 10px;">Semantic Segmentation</a> <a href="/blog/tags/entropy/" style="font-size: 10px;">entropy</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/11/">November 2015</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2017/10/23/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/blog/2016/07/24/è®ºæ–‡ç¬”è®°-ã€ŠFitNets-Hints-for-Thin-Deep-Netsã€‹/">è®ºæ–‡ç¬”è®° ã€ŠFitNets- Hints for Thin Deep Netsã€‹</a>
          </li>
        
          <li>
            <a href="/blog/2016/07/20/è®ºæ–‡ç¬”è®° ã€ŠDistilling the Knowledge in a Neural Networkã€‹/">è®ºæ–‡ç¬”è®° ã€ŠDistilling the Knowledge in a Neural Networkã€‹</a>
          </li>
        
          <li>
            <a href="/blog/2016/07/01/è®ºæ–‡ç¬”è®° ã€ŠFully Convolutional Networks for Semantic Segmentationã€‹/">è®ºæ–‡ç¬”è®° ã€ŠFully Convolutional Networks for Semantic Segmentationã€‹</a>
          </li>
        
          <li>
            <a href="/blog/2016/06/29/caffeä½¿ç”¨æ€»ç»“3/">Caffe ä½¿ç”¨æ€»ç»“ 3ï¼šæ•´ä½“è®­ç»ƒ</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/script.js"></script>

  </div>
</body>
</html>